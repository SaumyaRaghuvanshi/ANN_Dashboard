# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-v3SIXtwpl4sH_M4O6IOgSo4SVKL5yty
"""

import streamlit as st
import os
import gdown
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Streamlit UI
st.title('üìä Sales Prediction Dashboard - ANN Model')

# Upload dataset
TRAIN_FILE_ID = "1Isp2tA7MnXcNu9le5Lu7wwJNP7Kt67Ky"
STORE_FILE_ID = "1V8tjbvPiC0mI1AF4M4PajYPDt6orWv4e"
TRAIN_PATH = "train.csv"
STORE_PATH = "store.csv"

# Download the datasets automatically if not present
if not os.path.exists(TRAIN_PATH):
    gdown.download(f"https://drive.google.com/uc?id={TRAIN_FILE_ID}", TRAIN_PATH, quiet=False)

if not os.path.exists(STORE_PATH):
    gdown.download(f"https://drive.google.com/uc?id={STORE_FILE_ID}", STORE_PATH, quiet=False)

# Load datasets
train_df = pd.read_csv(TRAIN_PATH)
store_df = pd.read_csv(STORE_PATH)

# Merge the datasets on Store ID
df = train_df.merge(store_df, how="left", on="Store")

# Drop unnecessary columns
df.drop(columns=['Customers', 'PromoInterval'], inplace=True)

# Convert date column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Extract date features
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['WeekOfYear'] = df['Date'].dt.isocalendar().week

# Handle missing values
df['CompetitionDistance'].fillna(df['CompetitionDistance'].median(), inplace=True)
df['CompetitionOpenSinceMonth'].fillna(0, inplace=True)
df['CompetitionOpenSinceYear'].fillna(0, inplace=True)
df['Promo2SinceWeek'].fillna(0, inplace=True)
df['Promo2SinceYear'].fillna(0, inplace=True)

# Encoding categorical variables
cat_cols = ['StoreType', 'Assortment', 'StateHoliday']
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

# Scaling numerical features
num_cols = ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear',
            'Promo2SinceWeek', 'Promo2SinceYear', 'Year', 'Month', 'Day', 'WeekOfYear']

scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

# Show the pre-processed dataset
st.write("### üîç Pre-processed Dataset Preview:", df.head())

# Train-Test Split
X = df.drop(columns=['Sales', 'Date'])
y = df['Sales']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

st.write("‚úÖ Data Preprocessing Complete!")

# Sidebar for Hyperparameter Selection
st.sidebar.header("Model Hyperparameters")
num_layers = st.sidebar.slider("Number of Hidden Layers", 1, 5, 3)
neurons_per_layer = st.sidebar.slider("Neurons per Layer", 32, 256, 128)
activation = st.sidebar.selectbox("Activation Function", ['relu', 'tanh', 'sigmoid'])
dropout_rate = st.sidebar.slider("Dropout Rate", 0.0, 0.5, 0.3)
optimizer = st.sidebar.selectbox("Optimizer", ['adam', 'sgd', 'rmsprop'])
learning_rate = st.sidebar.slider("Learning Rate", 0.0001, 0.01, 0.001)
epochs = st.sidebar.slider("Number of Epochs", 10, 100, 50)

# Model Training Button
if st.button("üöÄ Train Model"):
    with st.spinner('Training ANN Model... Please wait!'):
        # Build ANN Model
        model = Sequential()
        model.add(Dense(neurons_per_layer, activation=activation, input_shape=(X_train.shape[1],)))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

        for _ in range(num_layers - 1):
            model.add(Dense(neurons_per_layer, activation=activation))
            model.add(BatchNormalization())
            model.add(Dropout(dropout_rate))

        model.add(Dense(1, activation='linear'))

        optimizer_dict = {"adam": Adam(learning_rate=learning_rate),
                          "sgd": SGD(learning_rate=learning_rate),
                          "rmsprop": RMSprop(learning_rate=learning_rate)}

        model.compile(loss='mse', optimizer=optimizer_dict[optimizer], metrics=['mae'])

        history = model.fit(X_train, y_train, validation_data=(X_test, y_test),
                            epochs=epochs, batch_size=64, verbose=1)

        st.success("üéâ Model Training Completed!")

        # Plot Loss
        fig, ax = plt.subplots()
        ax.plot(history.history['loss'], label='Training Loss')
        ax.plot(history.history['val_loss'], label='Validation Loss')
        ax.set_xlabel("Epochs")
        ax.set_ylabel("Loss")
        ax.legend()
        st.pyplot(fig)

        # Model Summary
        st.write("### üî• Model Summary")
        for layer in model.layers:
            output_shape = layer.output_shape if hasattr(layer, 'output_shape') else "N/A"
            st.write(f"Layer: {layer.name}, Output Shape: {output_shape}, Parameters: {layer.count_params()}")
